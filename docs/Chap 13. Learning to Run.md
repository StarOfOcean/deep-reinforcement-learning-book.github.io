---
layout: default
title: Chap 13. Learning to Run
nav_order: 13
---

## Authors

- Zihan Ding* - Princeton University (zhding[at]mail.ustc.edu.cn)
- Hao Dong - Peking University 

## Abstract

In this chapter, we provide a practical project for readers to have some hands-on experiences of deep reinforcement learning applications, in which we adopt one challenge hosted by crowdAI and NeurIPS 2017: *Learning to Run*. The environment has a 41-dimension state space and 18-dimension action space, both continuous, which is a moderately large-scale environment for novices to gain some experiences. We provide a soft actor-critic solution for solving the environment, as well as some tricks applied for boosting performances.

**Keywords**: learning to run, deep reinforcement learning, soft actor-critic, parallel training

## Code 

Codes for contents in this chapter are available [**here**](https://github.com/deep-reinforcement-learning-book/Chapter13-Learning-to-Run).

## Citation

To cite this book, please use this bibtex entry:

```
@incollection{deepRL-chapter13-2020,
 title={Learning to Run},
 chapter={13},
 author={Zihan Ding, Hao Dong},
 editor={Hao Dong, Zihan Ding, Shanghang Zhang},
 booktitle={Deep Reinforcement Learning: Fundamentals, Research, and Applications},
 publisher={Springer Nature},
 pages={367-378},
 note={\url{http://www.deepreinforcementlearningbook.org}},
 year={2020}
}
```



If you find any typos or have suggestions for improving the book, do not hesitate to contact with the corresponding author (name with *).